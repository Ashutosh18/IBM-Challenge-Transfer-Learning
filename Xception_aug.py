from keras import applications
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
from keras import optimizers
from keras.models import Sequential, Model
from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D
from keras import backend as k
from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping
import numpy as np
import os
import pandas as pd
from keras.preprocessing.image import ImageDataGenerator
import keras

img_width, img_height = 256, 256
data_augmentation = True
epochs=20

model = applications.xception.Xception(weights = "imagenet", include_top=False, input_shape = (img_width, img_height, 3))
for layer in model.layers[:-6]:
    layer.trainable = False

x = model.output
x = GlobalAveragePooling2D()(x)
#x = Flatten()(x)
x = Dense(1024, activation="relu")(x)
x = Dropout(0.5)(x)
x = Dense(1024, activation="relu")(x)
x = Dropout(0.5)(x)

predictions = Dense(25, activation="softmax")(x)

# creating the final model
model_final = Model(input = model.input, output = predictions)

# compile the model
model_final.compile(loss = "categorical_crossentropy", optimizer = optimizers.Adam(), metrics=["accuracy"])

y=[]
train=pd.read_csv('/ibm_data/train.csv')
X_train =[]

lab = list(train['label'].unique())

from sklearn.model_selection import train_test_split
y_train, y_test = train_test_split(train, train_size=0.9, test_size=0.1,stratify =train['label'])

y_test.reset_index(inplace = True, drop = True)
y_train.reset_index(inplace = True, drop= True)

train_dir = '/ibm_data/train_img/'
def data_load(data_dir,images):
    X_train=[]
    labels=[]
    for i in range(images.image_id.count()):
        img=load_img(data_dir+images['image_id'][i]+'.png')
        img=img_to_array(img)/255.0
        X_train.append(img)
        label=np.zeros(25)
        label[lab.index(images['label'][i])] = 1
        labels.append(label)
    X_train=np.asarray(X_train)
    y_train=np.asarray(labels)
    return X_train, y_train

X_train, y_train=data_load(train_dir,y_train)
X_test, y_test= data_load(train_dir,y_test)
batch_size = 128
#train_gen = data_gen_small(train_dir,y_train,batch_size)
#val_gen = data_gen_small(train_dir,y_val,batch_size)
if not data_augmentation:
    print('Not using data augmentation.')
    model_final.fit(x_train, y_train,
              batch_size=batch_size,
              epochs=epochs,
              validation_data=(X_test, y_test),
              shuffle=True)
else:
    print('Using real-time data augmentation.')
    # This will do preprocessing and realtime data augmentation:
    datagen = ImageDataGenerator(
        featurewise_center=False,  # set input mean to 0 over the dataset
        samplewise_center=False,  # set each sample mean to 0
        featurewise_std_normalization=False,  # divide inputs by std of the dataset
        samplewise_std_normalization=False,  # divide each input by its std
        zca_whitening=False,  # apply ZCA whitening
        rotation_range=30,  # randomly rotate images in the range (degrees, 0 to 180)
        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
        horizontal_flip=True,  # randomly flip images
        vertical_flip=True)  # randomly flip images

    # Compute quantities required for feature-wise normalization
    # (std, mean, and principal components if ZCA whitening is applied).
    datagen.fit(X_train)

    # Fit the model on the batches generated by datagen.flow().
    for i in range(5):
        print('For i=',i)
        optimizer=keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08, decay=0.0)
        model_final.optimizer.lr.assign(0.01 / 10 ** (i + 1))
        model_final.fit_generator(datagen.flow(X_train, y_train,
                                     batch_size=batch_size),
                        steps_per_epoch=X_train.shape[0]/batch_size,
                        epochs=epochs,
                        validation_data=(X_test, y_test))
#model_final.optimizer.lr.assign(0.0001)
#model_final.fit_generator(train_gen,steps_per_epoch=len(y_train)/batch_size,epochs=10,validation_data=val_gen,validation_steps=len(y_val)/batch_size)
test = pd.read_csv('/ibm_data/test.csv',index_col=False)

from tqdm import tqdm
print('Predicting on {} samples with batch_size = {}...'.format(len(test), 64))
pred =[]
for start in tqdm(range(0, len(test), batch_size)):
    x_batch = []
    end = min(start + batch_size, len(test))
    test_batch = test[start:end]
    for id in test_batch.values:
        img = load_img('/ibm_data/test_img/{}.png'.format(id[0]))
        img = img_to_array(img)
        x_batch.append(img)
    x_batch = np.array(x_batch, np.float32) / 255
    preds = model_final.predict_on_batch(x_batch)
    pred.extend(np.argmax(preds,axis=1))
    #print len(pred)

predtz=[]
for i in pred:
    predtz.append(lab[i])

print("Generating submission file...")
df = pd.DataFrame({'image_id': test['image_id'], 'label': predtz})
df.to_csv('/output/ibm_submission_ash.csv', index=False)