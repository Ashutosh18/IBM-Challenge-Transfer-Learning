from keras import applications
from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array
from keras import optimizers
from keras.models import Sequential, Model
from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D
from keras import backend as k
from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping
import numpy as np
import os
import pandas as pd
from keras.preprocessing.image import ImageDataGenerator

img_width, img_height = 256, 256
data_augmentation = True

model = applications.xception.Xception(weights = "imagenet", include_top=False, input_shape = (img_width, img_height, 3))
for layer in model.layers:
    layer.trainable = False

x = model.output
x = GlobalAveragePooling2D()(x)
#x = Flatten()(x)
x = Dense(1024, activation="relu")(x)
x = Dropout(0.5)(x)
x = Dense(1024, activation="relu")(x)
x = Dropout(0.5)(x)

predictions = Dense(25, activation="softmax")(x)

# creating the final model
model_final = Model(input = model.input, output = predictions)

# compile the model
model_final.compile(loss = "categorical_crossentropy", optimizer = optimizers.Adam(), metrics=["accuracy"])

y=[]
train=pd.read_csv('/ibm_data/train.csv')
X_train =[]

lab = list(train['label'].unique())

from sklearn.model_selection import train_test_split
y_train, y_val = train_test_split(train, train_size=0.8, test_size=0.2,stratify =train['label'])

y_val.reset_index(inplace = True, drop = True)
y_train.reset_index(inplace = True, drop= True)

#def HorizontalFlip(image):
#    if True:
#        image1 = image.transpose(Image.FLIP_LEFT_RIGHT)
#        image2 = image.transpose(Image.FLIP_TOP_BOTTOM)
#    return image1, image2

train_dir = '/ibm_data/train_img/'
def data_gen_small(data_dir,images,batch_size):
    while True:
        batch = np.random.choice(np.arange(len(images)),batch_size)
        imgs = []
        labels = []
        for i in batch:
            img = load_img(data_dir+ images['image_id'][i]+ '.png')
            #print(img.size)
            #img1,img2 = HorizontalFlip(img)
            array_img = img_to_array(img)/255.0
            #array_img1 = img_to_array(img1)/255.0
            #array_img2 = img_to_array(img2)/255.0
            imgs.append(array_img)
            #imgs.append(array_img1)
            #imgs.append(array_img2)
            label = np.zeros(25)
            label[lab.index(images['label'][i])] = 1
            labels.append(label)
            #labels.append(label)
            #labels.append(label)
            #print('Labels' + label)
            #print('img' + )

        imgs = np.array(imgs)
        labels = np.array(labels)
        yield imgs, labels

batch_size = 64
train_gen = data_gen_small(train_dir,y_train,batch_size)
val_gen = data_gen_small(train_dir,y_val,batch_size)
if not data_augmentation:
    print('Not using data augmentation.')
    model_final.fit(x_train, y_train,
              batch_size=batch_size,
              epochs=epochs,
              validation_data=(x_test, y_test),
              shuffle=True)
else:
    print('Using real-time data augmentation.')
    # This will do preprocessing and realtime data augmentation:
    datagen = ImageDataGenerator(
        featurewise_center=False,  # set input mean to 0 over the dataset
        samplewise_center=False,  # set each sample mean to 0
        featurewise_std_normalization=False,  # divide inputs by std of the dataset
        samplewise_std_normalization=False,  # divide each input by its std
        zca_whitening=False,  # apply ZCA whitening
        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)
        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)
        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)
        horizontal_flip=True,  # randomly flip images
        vertical_flip=True)  # randomly flip images

    # Compute quantities required for feature-wise normalization
    # (std, mean, and principal components if ZCA whitening is applied).
    datagen.fit(x_train)

    # Fit the model on the batches generated by datagen.flow().
    model.fit_generator(datagen.flow(x_train, y_train,
                                     batch_size=batch_size),
                        steps_per_epoch=x_train.shape[0] // batch_size,
                        epochs=epochs,
                        validation_data=(x_test, y_test))
#model_final.optimizer.lr.assign(0.0001)
#model_final.fit_generator(train_gen,steps_per_epoch=len(y_train)/batch_size,epochs=10,validation_data=val_gen,validation_steps=len(y_val)/batch_size)
for i in range(5):
    print('For i=', i)
    model_final.optimizer.lr.assign(0.1/10**(i+1))
    model_final.fit_generator(train_gen,steps_per_epoch=len(y_train)/128,epochs=10,validation_data=val_gen,validation_steps=len(y_val)/128)

'''test = pd.read_csv('test.csv',index_col=False)

from tqdm import tqdm
print('Predicting on {} samples with batch_size = {}...'.format(len(test), 64))
pred =[]
for start in tqdm(range(0, len(test), 64)):
    x_batch = []
    end = min(start + 64, len(test))
    test_batch = test[start:end]
    for id in test_batch.values:
        img = load_img('test_img/{}.png'.format(id[0]))
        img = img_to_array(img)
        x_batch.append(img)
    x_batch = np.array(x_batch, np.float32) / 255
    preds = model_final.predict_on_batch(x_batch)
    pred.extend(np.argmax(preds,axis=1))
    #print len(pred)

predtz=[]
for i in pred:
    predtz.append(lab[i])

print("Generating submission file...")
df = pd.DataFrame({'image_id': test['image_id'], 'label': predtz})
df.to_csv('ibm_submission1.csv', index=False)'''